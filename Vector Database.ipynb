{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Your Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, first create a python environment with `jupyter` installed. If you use conda, you can run a command like the one below\n",
    "\n",
    "```\n",
    "conda create -n vector-databases -yq python=3.12 jupyter \n",
    "```\n",
    "\n",
    "Once your environment is ready link it to the notebook and install the packages as we move on with the notebook. Alternatively, you can just install all packages in one go\n",
    "\n",
    "```\n",
    "pip install dotenv openai pinecone torch transformers open_clip_torch langchain-experimental\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Databases\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectors & Embeddings (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What is a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *vector* is a mathematical representation of data as an ordered list of numbers. In the context of *vector databases* and *machine learning*, a vector is used to represent features or characteristics of an object ‚Äî such as a word, image, or user ‚Äî in a numerical form that computers can efficiently process and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representing a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector is an element of an `n-dimensional` space (‚Ñù‚Åø), written as: ```V = [v1, v2, v3, ..., vn]```. Each component `ùë£ùëñ` is a real number representing a specific feature or dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Sentance Representation**\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/vec_rep1.jpeg\" alt=\"Alt text\" width=\"700\" height=\"200\" center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Image Representation**\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/vec_rep2.jpeg\" alt=\"Alt text\" width=\"700\" height=\"200\" center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Magniture & Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively: Think of a vector as a `point` or `arrow` in space. It has a direction and magnitude\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/vector-position-in-plane.png\" alt=\"Alt text\" width=\"500\" height=\"500\" center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A vector is not just a point** ‚Äî it‚Äôs a point relative to something (an arrow)\n",
    "\n",
    "if you only draw a vector as (3, 2), it looks like a single point. But mathematically, a vector represents movement or change ‚Äî not just a location.\n",
    "\n",
    "For example:\n",
    "  - The point (3, 2) can represent a location in space.\n",
    "  - But the vector (3, 2) represents ‚Äúgo 3 units right and 2 units up‚Äù ‚Äî a displacement from the origin (0, 0).\n",
    "\n",
    "So every vector has:\n",
    "- Direction ‚Üí where it points.\n",
    "- Magnitude ‚Üí how long it is (how far it goes).\n",
    "\n",
    "> In a high-dimensional space (when dimensions are >3) we stop drawing arrows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. What is an Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *embedding model* is a type of machine learning model that transforms complex, high-dimensional data ‚Äî such as `words`, `images`, or `documents` ‚Äî into dense `numerical vectors` (called embeddings) that *capture their meaning or relationships*.\n",
    "\n",
    "> **An Embedding** is the vector representation of an object (text, image, sound, etc.) in a continuous, lower-dimensional space.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/How-Embeddings-Work.jpg\" alt=\"Alt text\" width=\"500\" height=\"300\" center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does an Embedding Model needs to be Trained ?**\n",
    "\n",
    "The model learns to map similar things close together in vector space.\n",
    "\n",
    "For example:\n",
    "- Words like ‚Äúking‚Äù, ‚Äúqueen‚Äù, ‚Äúprince‚Äù end up close to each other.\n",
    "- Images of cats cluster together and are far from cars.\n",
    "- Customers with similar behaviors have nearby embeddings.\n",
    "\n",
    "So, distance between vectors reflects semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"imgs/word_embeddings_toy_example.png\" alt=\"Alt text\" width=\"500\" height=\"500\" center>\n",
    "</div>\n",
    "\n",
    "> Notice how cat and kitten are close ‚Äî because their meanings are related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Understanding `Embeddings` Vector Magnitude & Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"imgs/mag_direction.jpg\" alt=\"Alt text\" width=\"400\" height=\"300\" center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Magnitude**\n",
    "\n",
    "The magnitude (length) of a vector shows how much of something is present ‚Äî like intensity or confidence (i.e, how strongly something expresses certain features). Fromally, the magnitude of a vector is computed as $|\\vec{v}| = \\sqrt{x^2 + y^2 + z^2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Direction**\n",
    "\n",
    "The direction of a vector represents which features are active or what pattern it corresponds to. Think of direction as the identity of the concept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Embedding Models Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several embedding models available today, each representing a step forward in how machines understand and encode meaning. From simple statistical methods to deep contextual models, these embeddings transform raw data (like text or images) into numerical vectors that capture relationships and meaning. Below are some of the popular models. Each with its own story, architecture, and strengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec (2013, Google)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec was one of the first models to revolutionize natural language processing by demonstrating that words could be represented as dense vectors in a continuous vector space. This idea laid the foundation for modern embedding-based systems, including today‚Äôs vector databases.\n",
    "\n",
    "Word2Vec is known for its simplicity, interpretability, and computational efficiency. However, it produces static embeddings, meaning that each word has a single fixed vector regardless of context. Because embeddings are static, Word2Vec cannot capture word meaning based on context.\n",
    "\n",
    "For Example:\n",
    "- I deposited money in the bank\n",
    "- I sat by the river bank\n",
    "\n",
    "In Word2Vec, the word `bank` has one vector, even though the meanings are completely different.\n",
    "\n",
    "\n",
    "Modern embedding models - presented below - generate different vectors depending on context, which is critical for modern applications and search. Word2Vec is rarely used in modern applications, however it is mainly used for Educational purposes, Historical understanding of embeddings and Very lightweight or legacy systems where performance constraints outweigh semantic accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText (2016, Facebook AI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText builds on Word2Vec‚Äôs foundations by addressing one of its major limitations: out-of-vocabulary (OOV) and rare words. Instead of learning a single vector per word, FastText represents each word as a bag of character n-grams, allowing it to capture subword information such as prefixes, suffixes, and word stems.\n",
    "\n",
    "This approach makes FastText particularly robust for:\n",
    "- Morphologically rich languages\n",
    "- Spelling variations and typos\n",
    "- Previously unseen words\n",
    "\n",
    "For example: Even if the word ‚Äúunhappiness‚Äù was never seen during training, FastText can infer its meaning from subwords like:\n",
    "- ‚Äúun‚Äù\n",
    "- ‚Äúhappy‚Äù\n",
    "- ‚Äúness‚Äù\n",
    "\n",
    "Despite these improvements, FastText still produces static embeddings: The word ‚Äúapple‚Äù has the same vector in:\n",
    "- I ate an apple\n",
    "- Apple released a new iPhone\n",
    "\n",
    "It cannot model context-dependent meaning, which limits its effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT (2018, Google)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) marked a major leap in language representation. Unlike Word2Vec or FastText, BERT produces contextual embeddings, meaning the representation of a word depends on the entire sentence it appears in. This allows BERT to capture deep semantic and syntactic relationships between words.\n",
    "\n",
    "BERT is a foundation model for many NLP Tasks including \n",
    "- Question answering\n",
    "- Text classification\n",
    "- Chatbots\n",
    "- Semantic understanding\n",
    "\n",
    "While powerful, BERT is computationally intensive: It uses very high memory, provides slower inference and is not  optimized for large-scale embedding generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI Embeddings (2022‚Äìpresent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI‚Äôs embedding models (such as `text-embedding-3-small` and `text-embedding-3-large`) represent a modern generation of general-purpose, high-quality embeddings. They are trained on large, diverse datasets and explicitly optimized for semantic similarity, search, and clustering, making them particularly well suited for vector database applications.\n",
    "\n",
    "Unlike earlier embedding approaches that were task- or domain-specific, OpenAI embeddings capture a broad understanding of natural language across topics, writing styles, and use cases. They are designed to work well out of the box, without requiring additional fine-tuning or complex preprocessing.\n",
    "\n",
    "Why OpenAI Embeddings Are Widely Used\n",
    "- Strong performance on semantic retrieval tasks\n",
    "- Consistent vector quality across domains\n",
    "- Easy integration via a managed API\n",
    "- Optimized for production-scale systems\n",
    "\n",
    "These characteristics make them a common choice for:\n",
    "- Semantic search\n",
    "- Recommendation systems\n",
    "- Retrieval-Augmented Generation (RAG)\n",
    "- Reasoning over large document collections\n",
    "\n",
    "While OpenAI embeddings offer excellent quality and convenience, they come with important considerations:\n",
    "- Proprietary and API-based (no self-hosting)\n",
    "- Ongoing cost per request\n",
    "- Data leaves your infrastructure, which may be a concern for sensitive or regulated environments\n",
    "\n",
    "For these reasons, OpenAI embeddings are often contrasted with open-source alternatives such as BGE, where teams trade convenience for control and cost predictability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BGE (BAAI General Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BGE (BAAI General Embeddings) is a family of high-quality open-source embedding models developed by BAAI (Beijing Academy of Artificial Intelligence). BGE models are designed specifically for semantic search, retrieval, and RAG (Retrieval-Augmented Generation) tasks. They are widely used as drop-in alternatives to proprietary embeddings (e.g. OpenAI) in vector databases.\n",
    "\n",
    "**Key Characteristics**\n",
    "- Sentence & document embeddings\n",
    "- Optimized for retrieval (query ‚Üî document similarity)\n",
    "- Open-source and self-hostable\n",
    "- Strong performance on MTEB benchmarks\n",
    "- Available in multiple sizes and languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLIP (Contrastive Language‚ÄìImage Pretraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP is a multimodal embedding model developed by OpenAI that learns joint representations of text and images. Unlike text-only embedding models, CLIP can work with both Text and Images; embedding both into the same vector space. This enables cross-modal search.\n",
    "\n",
    "CLIP is a proprietary model, belongs to Open AI. However, Open-source variants also exist like `OpenCLIP` and `LAION CLIP`\n",
    "\n",
    "\n",
    "An embedding model like CLIP can be used to support Usecases like\n",
    "- Image search engines\n",
    "- E-commerce product search\n",
    "- Visual recommendation systems\n",
    "- Content moderation\n",
    "- Multimedia knowledge bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Distance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"imgs/DistanceMeasuresEx.png\" alt=\"Alt text\" width=\"700\" height=\"300\" center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures how similar the direction of two vectors is ‚Äî not their length, i.e. the angle between two vectors. The cosine similarity between two vectors **A** and **B** is given by:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\, \\|B\\|}\n",
    "$$\n",
    "\n",
    "\n",
    "**Range:** \n",
    "\n",
    "‚àí 1 to +1\n",
    "\n",
    "**Meaning of values:**\n",
    "| Value  | Meaning                                                     |\n",
    "| :----- | :---------------------------------------------------------- |\n",
    "| **+1** | Vectors point in the **same direction** (identical meaning) |\n",
    "| **0**  | Vectors are **orthogonal** (no relationship / unrelated)    |\n",
    "| **-1** | Vectors point in **opposite directions** (opposite meaning) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def dot_product(a, b):\n",
    "    \"\"\"Compute the dot product of two equal-length vectors.\"\"\"\n",
    "    return sum(x * y for x, y in zip(a, b))\n",
    "\n",
    "def norm(v):\n",
    "    \"\"\"Compute the magnitude (Euclidean norm) of a vector.\"\"\"\n",
    "    return math.sqrt(sum(x**2 for x in v))\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors a and b.\n",
    "    Uses helper functions for clarity.\n",
    "    \"\"\"\n",
    "    mag_a = norm(a)\n",
    "    mag_b = norm(b)\n",
    "\n",
    "    # Handle division by zero (if one vector is all zeros)\n",
    "    if mag_a == 0 or mag_b == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return dot_product(a, b) / (mag_a * mag_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures how far apart two vectors are in space, considering both direction and length, i.e. the straight-line distance between two points.  The Euclidean distance between two vectors **A** and **B** is given by:\n",
    "\n",
    "$$\n",
    "d(A, B) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\n",
    "$$\n",
    "\n",
    "**Range**:\n",
    "\n",
    "0 to +‚àû\n",
    "\n",
    "**Meaning of values:**\n",
    "| Value             | Meaning                                      |\n",
    "| :---------------- | :------------------------------------------- |\n",
    "| **0**             | Vectors are **identical**                    |\n",
    "| **Larger number** | Vectors are **further apart** (less similar) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_vectors(a, b):\n",
    "    \"\"\"Subtract vector b from vector a (element-wise).\"\"\"\n",
    "    return [x - y for x, y in zip(a, b)]\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two vectors a and b.\n",
    "    \"\"\"\n",
    "    diff = subtract_vectors(a, b)\n",
    "    return norm(diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"imgs/points_graph1.png\" alt=\"Alt text\" width=\"500\" height=\"500\" center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,1]\n",
    "b = [2,2]\n",
    "c = [-1,-1]\n",
    "\n",
    "print(cosine_similarity(a, a))\n",
    "\n",
    "\n",
    "# A and B point in the same direction ‚Üí cosine similarity = 1\n",
    "# Even though B is twice as long, it‚Äôs still perfectly aligned with A, so cosine similarity = 1.\n",
    "print(cosine_similarity(a, b))\n",
    "\n",
    "# A and C point in opposite directions ‚Üí cosine similarity = -1\n",
    "print(cosine_similarity(a, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(euclidean_distance(a, a))\n",
    "\n",
    "\n",
    "# A and B are close (same direction, similar position)\n",
    "print(euclidean_distance(a, b))\n",
    "\n",
    "\n",
    "# A and C are far (different direction)\n",
    "print(euclidean_distance(a, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Use Case                         | Metric                 | Why                                                                                                           |\n",
    "| :------------------------------- | :--------------------- | :------------------------------------------------------------------------------------------------------------ |\n",
    "| **Semantic search / embeddings** | **Cosine similarity**  | Focuses on meaning (direction). Magnitude differences are irrelevant because embeddings are often normalized. |\n",
    "| **Clustering numeric data**      | **Euclidean distance** | Distance in scale matters (e.g., prices, coordinates).                                                        |\n",
    "| **Physics or geometry problems** | **Euclidean distance** | Actual spatial distances are meaningful.                                                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Vectors & Embeddings (Practical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Using Open AI Embedding Models: `text-embeddings-3-small/large`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sub-section, we demonstrate how to generate embeddings using OpenAI‚Äôs proprietary models. These models are hosted on OpenAI‚Äôs servers and provide high-quality, ready-to-use embeddings without any local setup. While convenient and reliable, they require an API key and depend on external infrastructure for all requests. Before you move on, you first need to setup your OpenAI API Key from [OpenAI's platform](https://platform.openai.com/). Once you have your API Key, you can set it up in your `.env` file or directly provide it in your notebook as we will see in the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading OPENAI_API_KEY from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# alternatively, provde your api key directly into the notebook\n",
    "# OPENAI_API_KEY = \"YOUR API KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",   # or \"text-embedding-3-small\"\n",
    "    input=\"I will deposite money in the bank\"\n",
    ")\n",
    "\n",
    "print(resp.data[0].embedding)\n",
    "print(len(resp.data[0].embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentences_open_ai(sentences):\n",
    "\n",
    "    # Request embeddings\n",
    "    resp = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",   # or \"text-embedding-3-small\"\n",
    "        input=sentences\n",
    "    )\n",
    "\n",
    "    # extract embeddings from response and \n",
    "    # return in a list of embeddings\n",
    "    embeddings = list()\n",
    "    for item in resp.data:\n",
    "        embeddings.append(item.embedding)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Using Open Source Embedding Models: An example with `bge-large-en-v1.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Proprietary AI models`, such as those offered by **OpenAI** and similar vendors, have played a significant role in accelerating the adoption of large-scale language and multimodal systems. However, relying on closed, hosted models introduces several structural drawbacks that become increasingly relevant as AI systems move from experimentation to production. **First**, proprietary models operate as black boxes: their architectures, training data, and optimization strategies are not fully disclosed. This limits transparency, makes debugging and auditing difficult, and creates challenges for compliance, reproducibility, and long-term maintainability. **Second**, usage is typically bound to external APIs, introducing latency, availability dependencies, and vendor lock-in. Cost can also scale unpredictably with usage, making budgeting and optimization harder at scale. **Finally**, data privacy and governance constraints may prevent sensitive or regulated data from being processed by third-party services altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, `open-source models` hosted on platforms like **Hugging Face** provide a compelling alternative. These models can be downloaded, inspected, fine-tuned, and deployed entirely within private infrastructure. This enables full control over data flow, inference costs, and system behavior, while eliminating external dependencies at runtime. The Hugging Face ecosystem also offers a rich and rapidly evolving catalog of high-quality models for text, image, and multimodal tasks‚Äîmany of which rival or exceed proprietary solutions in specific domains such as semantic similarity, retrieval, and embeddings.\n",
    "\n",
    "Crucially, modern open models (e.g.,`BGE`, `E5`, `Nomic`) are designed to be drop-in replacements for common proprietary use cases. They integrate seamlessly with standard tooling, support offline execution after initial download, and allow teams to build scalable, production-grade AI systems without sacrificing performance or control. Open-source models hosted on Hugging Face are increasingly becoming the default choice for organizations seeking transparency, cost efficiency, and long-term architectural flexibility - without compromising on quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Text Embeddings with `bge-large-en-v1.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of using locally hosted, open-source models, we will work with the bge-large-en-v1.5 model from BAAI‚Äôs BGE (Beijing General Embeddings) family. BGE models are specifically designed for generating high-quality sentence embeddings, making them well-suited for semantic search and other similarity-based tasks. Unlike proprietary models such as OpenAI‚Äôs embeddings, these models can be hosted locally - on your own server or even your personal machine - without requiring an account on any external platform. They are fully open-source and free to use.\n",
    "\n",
    "In the next few cells, we will demonstrate how to host `bge-large-en-v1.5` locally and create an embed_sentences function as a drop-in alternative to the embeddings function we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have an NVIDIA GPU (recommended)**\n",
    "\n",
    "Install CUDA-enabled PyTorch from\n",
    "üëâ https://pytorch.org/get-started/locally/\n",
    "\n",
    "> Example:\n",
    "> \n",
    "> pip install torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we download the model files from `Hugging Face` Server. After the first download, the model runs fully locally/offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -----------------------------\n",
    "# Model configuration\n",
    "# -----------------------------\n",
    "# Hugging Face model name\n",
    "# BAAI: the organization/user that published the model\n",
    "# bge-large-en-v1.5: the specific model version (BGE = Beijing General Embedding, large English model, v1.5)\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "# Select device:\n",
    "# - \"cuda\" if GPU is available\n",
    "# - otherwise fall back to CPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load tokenizer and model\n",
    "# -----------------------------\n",
    "# NOTE:\n",
    "# - The first run will download the model\n",
    "# - After that, everything runs fully offline\n",
    "# - Files are cached in ~/.cache/huggingface/\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "# (important: disables dropout, improves consistency)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence_bge(sentence: str) -> list:\n",
    "    \"\"\"\n",
    "    Convert a single sentence into a normalized embedding vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        The input sentence to embed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of floats representing the sentence embedding.\n",
    "        (Length = 1024 for bge-large-en-v1.5)\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    # - padding/truncation ensure safe input length\n",
    "    # - return_tensors=\"pt\" returns PyTorch tensors\n",
    "    inputs = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Move input tensors to the same device as the model\n",
    "    inputs = {key: value.to(DEVICE) for key, value in inputs.items()}\n",
    "\n",
    "    # Disable gradient calculation (faster + less memory)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # BGE models use the CLS token (first token) as sentence embedding\n",
    "        # Shape: [1, hidden_size]\n",
    "        embedding = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        # Normalize embedding to unit length\n",
    "        # This is critical for cosine similarity search\n",
    "        embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "    # Convert from:\n",
    "    # PyTorch tensor -> CPU -> NumPy -> Python list\n",
    "    return embedding[0].cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I will deposite money in the bank\"\n",
    "embedding = embed_sentence_bge(sentence)\n",
    "print(len(embedding))\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentences_bge(sentences):\n",
    "    embeddings = list()\n",
    "    for sentence in sentences:\n",
    "        embeddings.append(\n",
    "            embed_sentence_bge(sentence)\n",
    "        )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Open-Source Models for Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While BGE provides a robust example of locally hosted sentence embeddings, it is by no means the only option. The Hugging Face ecosystem hosts a wide variety of open-source models for text, image, and multimodal embeddings, many of which are fully compatible with the same workflow we demonstrated.\n",
    "\n",
    "These models can serve as drop-in replacements: in most cases, all you need to do is change the model name in your code, and the existing embedding functions, pooling, and normalization steps will continue to work seamlessly.\n",
    "\n",
    "Below are some notable examples organized by embedding type, but you can view the full list of models [here](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Embeddings**\n",
    "| Model                                     | Notes                                                           |\n",
    "| ----------------------------------------- | --------------------------------------------------------------- |\n",
    "| `BAAI/bge-large-en-v1.5`                  | High-quality, English sentence embeddings, local hosting        |\n",
    "| `intfloat/e5-large-v2`                    | Excellent retrieval performance, optimized for semantic search  |\n",
    "| `nomic-ai/nomic-embed-text-v1.5`          | LLaMA-based, high-quality embeddings, supports local deployment |\n",
    "| `sentence-transformers/all-mpnet-base-v2` | Lightweight and fast, widely used in semantic similarity tasks  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Embeddings**\n",
    "| Model                                   | Notes                                                                     |\n",
    "| --------------------------------------- | ------------------------------------------------------------------------- |\n",
    "| `openai/clip-vit-large-patch14`         | Gold standard for image embeddings, compatible with text-image similarity |\n",
    "| `openai/clip-vit-base-patch32`          | Smaller and faster alternative                                            |\n",
    "| `laion/CLIP-ViT-H-14-laion2B-s32B-b79K` | Strong open-source CLIP model for image embeddings                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text ‚Üî Image Embeddings (Multimodal)**\n",
    "| Model                                   | Notes                                                                               |\n",
    "| --------------------------------------- | ----------------------------------------------------------------------------------- |\n",
    "| `openai/clip-vit-large-patch14`         | Text and image share the same vector space, ideal for retrieval and semantic search |\n",
    "| `openai/clip-vit-base-patch32`          | Faster and smaller multimodal model                                                 |\n",
    "| `laion/CLIP-ViT-B-32-laion2B-s34B-b79K` | OpenCLIP variant, high-quality open-source alternative                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Normalizing Embeddings Before Using Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with text embeddings, similarity is usually measured using `cosine similarity`, because we care about `semantic meaning`, not vector magnitude. It is always preferred to **normalize** embeddings first, so that similarity depends only on meaning and not on arbitrary differences in vector length. With normalization, we basically scale the values of embeddings (i.e. features values) so that the magnitude of the corresponding vector (i.e. its length) becomes exactly 1. \n",
    "\n",
    "$$\n",
    "\\| \\mathbf{v} \\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical implication for Normalizing Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Comparison between any two embeddings depends only on their angle, i.e., how close their directions are (Pure semantic comparison)\n",
    "  - This makes cosine similarity equal to the dot product, simplifying computation and guaranteeing stable similarity values in the range [-1, 1]\n",
    "\n",
    "  - Euclidean distance can still be computed but gives almost the same ranking (because all vectors are unit-length).\n",
    "  - Magnitude-based metrics (like ‚Äúintensity of meaning‚Äù) are not available from these embeddings\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\, \\|B\\|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Normalized Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With BGE-style models, embeddings are not normalized by default, so we explicitly normalize them ourselves. In contrast, OpenAI embedding models (e.g. text-embedding-3-small, text-embedding-3-large) return unit-length embeddings automatically. This normalization happens out of the box, without extra steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_sentences = embed_sentences_open_ai\n",
    "# embed_sentences = embed_sentences_bge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"happy\", \"joyful\", \"cat\", \"dog\", \"king\", \"queen\"]\n",
    "embeddings = embed_sentences(words)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    word_norm = round(norm(embedding), 2)\n",
    "    print(f\"The magnitude of the word: {word} is {word_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Practical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_sentences = embed_sentences_bge\n",
    "embed_sentences = embed_sentences_open_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"happy\", \"joyful\", \"cat\", \"dog\", \"king\", \"queen\"]\n",
    "embeddings = embed_sentences(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_measure(embedding, other_embeddings, distance_measure_fn):\n",
    "    distance_measures = list()\n",
    "    for other_embedding in other_embeddings:\n",
    "        distance_measures.append(round(distance_measure_fn(embedding, other_embedding), 2))\n",
    "    return distance_measures\n",
    "\n",
    "get_distance_measure(embedding=embeddings[0], other_embeddings=embeddings, distance_measure_fn=euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "header = \"\\t\".join(f\"{w:>10}\" for w in words)\n",
    "print(f\"{'':>10}\\t{header}\")\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    distance_arr = get_distance_measure(\n",
    "        embedding, embeddings, distance_measure_fn=cosine_similarity\n",
    "    )\n",
    "    row = \"\\t\".join(f\"{d:>10.4f}\" for d in distance_arr)\n",
    "    print(f\"{words[i]:>10}\\t{row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance\n",
    "header = \"\\t\".join(f\"{w:>10}\" for w in words)\n",
    "print(f\"{'':>10}\\t{header}\")\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    distance_arr = get_distance_measure(\n",
    "        embedding, embeddings, distance_measure_fn=euclidean_distance\n",
    "    )\n",
    "    row = \"\\t\".join(f\"{d:>10.4f}\" for d in distance_arr)\n",
    "    print(f\"{words[i]:>10}\\t{row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Vector Databases (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector database stores and manages high-dimensional vector data. Data points are stored as arrays of numbers called **vectors** which are organized (`indexed`) in a way that allows for efficient, low-latency similarity based queries\n",
    "\n",
    "> Vector databases are growing in popularity because they deliver the speed and performance needed to drive generative artificial intelligence (AI) use cases and applications. According to Gartner¬Æ, by 2026, more than 30% of enterprises will have adopted vector databases to build their foundation models with relevant business data [check](https://www.gartner.com/account/signin?method=initialize&TARGET=http%3A%2F%2Fwww.gartner.com%2Fdocument%2F4705699%3Fref%3DTypeAheadSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The applications for vector databases are vast and growing. Some key use cases include:\n",
    "\n",
    "- Retrieval-augmented generation (RAG)\n",
    "- Conversational AI\n",
    "- Recommendation engines\n",
    "- Vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Vector databases versus traditional databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nature of data has undergone a profound transformation. It's no longer confined to structured information easily stored in traditional databases. Unstructured data ‚Äî including social media posts, images, videos, audio clips and more ‚Äî is growing 30% to 60% year over year\n",
    "\n",
    "While Relational databases excel at managing structured and semistructured datasets in specific formats, it is not optimized and does not fit use for un-structured data (ex. Documents, Audio Files, ...etc.). \n",
    "\n",
    "Unlike traditional relational databases with rows and columns, data points in a vector database are represented by vectors with a fixed number of dimensions. These are basically un-structured data transformed into embeddings. Because they use high-dimensional vector embeddings, vector databases are better able to handle unstructured datasets.\n",
    "\n",
    "> Vectors can represent complex objects such as words, images, videos and audio ... and are typically generated by an ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional search typically represents data by using discrete tokens or features, such as keywords, tags or metadata. Traditional searches rely on exact matches to retrieve relevant results. For example, a search for \"smartphone\" would return results containing the word \"smartphone.\"\n",
    "\n",
    "Opposed to this, vector search represents data as dense vectors, which are vectors with most or all elements being nonzero. Vectors are represented in a continuous vector space, the mathematical space in which data is represented as vectors. The vector representations enable similarity search. For example, a vector search for ‚Äúsmartphone‚Äù might also return results for ‚Äúcellphone‚Äù and ‚Äúmobile devices.‚Äù\n",
    "\n",
    "> Each dimension of the dense vector corresponds to a latent feature or aspect of the data. A latent feature is an underlying characteristic or attribute that is not directly observed but inferred from the data through mathematical models or algorithms. Latent features capture the hidden patterns and relationships in the data, enabling more meaningful and accurate representations of items as vectors in a high-dimensional space.\n",
    "\n",
    "> When we query the scalar index to retrieve rows or records, we generally query for exact matches. The power of indexes using vector embeddings that capture semantic information is we can instead search the index for approximate matches. We provide a vector as input and ask the vector index to return other vectors similar to the input or query vector. This allows us to search large datasets of vectors very quickly. The class of algorithms used to build and search vector indexes is called Approximate Nearest Neighbor (ANN) search. ANN algorithms rely on a similarity measure to determine the nearest neighbors. The vector index must be constructed based on a particular similarity metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"imgs/1_4l3TBZGVwRpH8o0pGMSbJg.gif\" alt=\"Alt text\" width=\"500\" height=\"500\" center>\n",
    "</div>\n",
    "\n",
    "Image source: [Medium post](https://medium.com/@tushar_datascience/12-types-of-databases-you-must-know-in-2025-a-complete-guide-1586d3df19cb) by Tushar Mahuri (2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Components of a Vector Database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector databases store the outputs of an embedding model algorithm, the vector embeddings. They also store each vector‚Äôs metadata‚Äîincluding; for example, a title, description and original data type. These can be queried by using metadata filters.\n",
    "\n",
    "By ingesting and storing these embeddings, the database can facilitate fast retrieval of a similarity search, matching the user‚Äôs prompt with a similar vector embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors need to be indexed to accelerate searches within high-dimensional data spaces. Vector databases create indexes on vector embeddings for search functions. Indexing maps the vectors to new data structures that enable faster similarity or distance searches, such as nearest neighbor searches, between vectors.\n",
    "\n",
    "There are different techniques that can be used for indexing. Vectors can be indexed by using algorithms such as hierarchical navigable small world (HNSW), locality-sensitive hashing (LSH) or product quantization (PQ).\n",
    "\n",
    "\n",
    "> Vector databases use indexing techniques to enable faster searching. Vector indexing and distance-calculating algorithms such as nearest neighbor search can help optimize performance when searching for relevant results across large datasets with millions, if not billions, of data points. One consideration is that with indexing, vector databases provide `approximate` results. Applications requiring greater accuracy might need to use no or flat indexing or a different kind of database at the cost of a slower processing speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity search based on querying or prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a user queries a vector database, first the vector embedding representation of the query is computed (Using the same embedding model / transformer of the original data that was stored and indexed in the database). The database then calculates distances between query vectors and vectors stored in the index to return similar results.\n",
    "\n",
    "Databases can measure the distance between vectors with various algorithms, such as nearest neighbor search. Measurements can also be based on various similarity metrics, such as cosine similarity.\n",
    "\n",
    "The database then returns the most similar vectors or nearest neighbors to the query vector according to the similarity ranking.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/Vector Databases.drawio.png\" alt=\"Alt text\" width=\"550\" height=\"200\" center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Vector Indexes (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. How does a vector index work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An index in a database is a data structure that helps the system find rows quickly without scanning the entire table‚Äîmuch like the index of a book. Indexes make read queries faster, though they use extra space and can slightly slow down writes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Database Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditional databases, an index is built on scalar data‚Äîcolumns with single values, such as numbers, dates, or strings. Each row represents a fact or object, and columns describe its attributes or link to other tables. Queries on a traditional index usually look for exact matches (e.g., find all users with age = 30)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector index is designed for high-dimensional vector data, such as embeddings produced by models like BERT, BGE, or OpenAI embeddings. Vector indexes enable fast and accurate similarity search and retrieval of vector embeddings from a large dataset of objects.\n",
    "\n",
    "The class of algorithms used to build and search vector indexes is called **Approximate Nearest Neighbor (ANN) search**. ANN algorithms rely on a similarity measure to determine the nearest neighbors. The vector index must be constructed based on a particular similarity metric. \n",
    "\n",
    "Approximate Nearest Neighbor (ANN) algorithms achieve high performance by trading perfect accuracy for speed and scalability. Instead of guaranteeing the exact closest vectors, ANN methods return vectors that are very likely to be among the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Flat indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flat indexing is an index strategy where we store each vector as is, with no modifications. This approach is easy to implement, and provides perfect accuracy. The downside is it is slow. In a flat index, the similarity between the query vector and every other vector in the index is computed.\n",
    "\n",
    "We then return the `K` vectors with the smallest similarity score.\n",
    "\n",
    "Flat indexing is the right choice when perfect accuracy is required and speed is not a consideration.  If the dataset we are searching is small, flat indexing may be a good choice as the search speed can still be reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Locality Sensitive Hashing (LSH) indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locality Sensitive Hashing is an indexing strategy that optimizes for speed and finding an `approximate` nearest neighbor, instead of doing an exhaustive search to find the actual nearest neighbor as is done with flat indexing.\n",
    "\n",
    "The index is built using a hashing function. Vector embeddings that are near each other are hashed to the same bucket. We can then store all these similar vectors in a single table or bucket.\n",
    "\n",
    "When a query vector is provided, its nearest neighbors can be found by hashing the query vector, and then computing the similarity metric for all the vectors in the table for all other vectors that hashed to the same value. This results in a much smaller search compared to flat indexing where the similarity metric is computed over the whole space, greatly increasing the speed of the query.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/Locality-sensitive-hashing-LSH.png\" alt=\"Alt text\" width=\"300\" height=\"300\" center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Inverted file (IVF) indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverted file (IVF) indexes are similar to LSH in that the goal is to first map the query vector to a smaller subset of the vector space and then only search that smaller space for approximate nearest neighbors.  This will greatly reduce the number of vectors we need to compare the query vector to‚Äìthus speeding up our ANN search.\n",
    "\n",
    "In LSH that subset of vectors was produced by a hashing function. In IVF, the vector space is partitioned or clustered, and then centroids of each cluster are found. For a given query vector, we then find the closest centroid. Then for that centroid, we search all the vectors in the associated cluster.\n",
    "\n",
    "Note that there is a potential problem when the query vector is near the edge of multiple clusters. In this case, the nearest vectors may be in the neighboring cluster. In these cases, we generally need to search multiple clusters.\n",
    "\n",
    "\n",
    "> IVF splits the whole data into several clusters using techniques like K-means clustering. Each vector of the database is assigned to a specific cluster. When a new query comes, the system doesn‚Äôt traverse the whole dataset. Instead, it identifies the nearest or most similar clusters and searches for the specific document within those clusters.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/0_MB4pj6br37hutgCu.webp\" alt=\"Alt text\" width=\"400\" height=\"350\" center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Hierarchical Navigable Small Worlds (HNSW) indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Navigable Small World (HNSW) is one of the most popular algorithms for building a vector index. It is very fast and efficient. \n",
    "\n",
    "HNSW is a multi-layered graph approach to indexing data. At the lowest level, every vector in the index is captured.  As we move up layers in the graph, data points are grouped based on similarity to reduce the number of data points in each layer exponentially.  In a single layer, points are connected based on their similarity.  Data points in each layer are also connected to those in the next layer.\n",
    "\n",
    "To search the index, we first search for the highest layer of the graph. The closest match from this graph is then taken to the next layer down where we again find the closest matches to the query vector. We continue this process until we reach the lowest layer in the graph.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/2d5d5f8b5aa8f575398f16cef31467dc95e4ed8b-3080x2136.avif\" alt=\"Alt text\" width=\"500\" height=\"350\" center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many considerations for choosing an appropriate vector index strategy. First, we have use case considerations. How fast do you need the results? How accurate do you need them to be? All vector index methods have some balance between the speed at which we can retrieve data and find similar vectors and how accurate the results will be.\n",
    "\n",
    "Additional challenges that are encountered in indexes is how much memory is used. Different algorithms may greatly increase the amount of data that needs to be stored to run the ANN searches efficiently.\n",
    "\n",
    "The index must also be built before we can start executing queries against it. Considerations for how complex it is to build the index need to be considered. Also, how difficult is it to update the index when new vectors are added? If we have to recompute the entire index each time a new vector is added, we need to ask ourselves how often our index will be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Implementing a Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Implementing an LSH Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_vector(dim):\n",
    "    \"\"\"Generate a random vector with components in [-1, 1].\"\"\"\n",
    "    return [random.uniform(-1, 1) for _ in range(dim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSH Index Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "num_tables = 2\n",
    "num_hashes = 3\n",
    "dim = 3\n",
    "# ----\n",
    "tables = dict()\n",
    "for i in range(num_tables):\n",
    "    tables[i] = {\n",
    "        \"planes\": [random_vector(dim) for _ in range(num_hashes)],\n",
    "        \"data\": defaultdict(list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sign Projection (Hashing Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - For each hyperplane, we calculate the `dot product` between our vector and the hyperplane vector.\n",
    " - If it‚Äôs `positive`, the vector lies on one side ‚Üí output '1'.\n",
    " - If it‚Äôs `negative`, the vector lies on the other side ‚Üí output '0'.\n",
    " - The sequence of bits (like `10110010`) becomes a `hash key` ‚Äî meaning all vectors with the same key are likely to be close in angle (similar direction).\n",
    "\n",
    "> We use DOT PRODUCT, not COSINE SIMILARITY, because the purpose is not to measure how similar two vectors are (`vec`, `hp`). Instead, it‚Äôs to decide which side of a random hyperplane (hp) a vector (vec) lies on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(vec, hyperplanes):\n",
    "    bits = ['1' if dot_product(vec, hp) >= 0 else '0' for hp in hyperplanes]\n",
    "    return ''.join(bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSH Insert "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each table:\n",
    " - Compute the hash (binary string) for the vector.\n",
    " - Store (id, vector) in that bucket.\n",
    "This way, similar vectors will share the same or nearby hash keys and end up in the same buckets.\n",
    "\n",
    "> üí° Example:\n",
    "> If two vectors produce the same hash '10110001' in a table, they‚Äôll be stored together in the same list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh_insert(id, vector):\n",
    "    for i in range(num_tables):\n",
    "        table = tables[i]\n",
    "        h = get_hash(vector, table[\"planes\"])\n",
    "        table[\"data\"][h].append({\"id\": id, \"vector\": vector})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSH Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute hash for the query vector in each table.\n",
    "- Gather all items stored in those hash buckets (these are candidate neighbors).\n",
    "- Remove duplicates using a set.\n",
    "- Compute cosine similarity between the query and each candidate.\n",
    "- Sort by similarity and return the top-k most similar ones.\n",
    "\n",
    "> üí° Why:\n",
    "> LSH doesn‚Äôt give the exact nearest neighbors, but it gives a small subset of likely candidates that you can then rank exactly using cosine similarity. That‚Äôs why it‚Äôs much faster than checking all vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh_query(query_vec, top_k=3):\n",
    "    \"\"\"Query nearest neighbors of query_vec.\"\"\"\n",
    "    candidates = dict()\n",
    "    for i in range(num_tables):\n",
    "        table = tables[i]\n",
    "        h = get_hash(query_vec, table[\"planes\"])\n",
    "        for candidate_data in table[\"data\"].get(h, list()):\n",
    "            candidate_id = candidate_data[\"id\"]\n",
    "            candidates[candidate_id] = candidate_data\n",
    "\n",
    "    n_candidates = len(candidates.keys())\n",
    "    print(f\"The number of candidates to be searched is {n_candidates}\")\n",
    "\n",
    "    if n_candidates == 0:\n",
    "        return list()\n",
    "        \n",
    "    sims = list()\n",
    "    for candidate_id, candidate_data in candidates.items():\n",
    "        sims.append({\n",
    "            \"id\": candidate_id,\n",
    "            \"similarity\": cosine_similarity(query_vec, candidate_data[\"vector\"]),\n",
    "        })\n",
    "\n",
    "    sims.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    return sims[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSH Index Implementation as a Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self, dim, num_hashes=3, num_tables=2):\n",
    "        self.dim = dim\n",
    "        self.num_hashes = num_hashes\n",
    "        self.num_tables = num_tables\n",
    "        self.tables = self._init_tables()\n",
    "\n",
    "    def _init_tables(self):\n",
    "        tables = dict()\n",
    "        for i in range(self.num_tables):\n",
    "            tables[i] = {\n",
    "                \"planes\": [random_vector(self.dim) for _ in range(self.num_hashes)],\n",
    "                \"data\": defaultdict(list)\n",
    "            }\n",
    "        return tables\n",
    "\n",
    "    def _get_hash(self, vec, hyperplanes):\n",
    "        bits = ['1' if dot_product(vec, hp) >= 0 else '0' for hp in hyperplanes]\n",
    "        return ''.join(bits)\n",
    "\n",
    "    def insert(self, id, vector):\n",
    "        for i in range(self.num_tables):\n",
    "            table = self.tables[i]\n",
    "            h = self._get_hash(vector, table[\"planes\"])\n",
    "            table[\"data\"][h].append({\"id\": id, \"vector\": vector})\n",
    "            \n",
    "\n",
    "    def query(self, query_vec, top_k=3):\n",
    "\n",
    "        candidates = dict()\n",
    "        for i in range(self.num_tables):\n",
    "            table = self.tables[i]\n",
    "            h = self._get_hash(query_vec, table[\"planes\"])\n",
    "            for candidate_data in table[\"data\"].get(h, list()):\n",
    "                candidate_id = candidate_data[\"id\"]\n",
    "                candidates[candidate_id] = candidate_data\n",
    "\n",
    "        n_candidates = len(candidates.keys())\n",
    "        print(f\"The number of candidates to be searched is {n_candidates}\")\n",
    "\n",
    "        if n_candidates == 0:\n",
    "            return list()\n",
    "            \n",
    "        sims = list()\n",
    "        for candidate_id, candidate_data in candidates.items():\n",
    "            sims.append({\n",
    "                \"id\": candidate_id,\n",
    "                \"similarity\": cosine_similarity(query_vec, candidate_data[\"vector\"]),\n",
    "            })\n",
    "\n",
    "        sims.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        return sims[:top_k]\n",
    "    \n",
    "    def explain(self):\n",
    "        for i in range(self.num_tables):\n",
    "\n",
    "            table = self.tables[i]\n",
    "            print(f\"Table {i+1}:\")\n",
    "\n",
    "            for h, candidates in table[\"data\"].items():\n",
    "                print(f\"  Hash: {h} --> contains {len(candidates)} candidates\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Lets Try Our LSH Hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randonly generating vector data \n",
    "dim = 16\n",
    "vectors = {f\"vec{i}\": [random.uniform(-1, 1) for _ in range(dim)] for i in range(5000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lsh Index and insert data\n",
    "lsh_index = LSH(dim=dim, num_hashes=4, num_tables=3)\n",
    "for id, vec in vectors.items():\n",
    "    lsh_index.insert(id, vec)\n",
    "lsh_index.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query a near-duplicate\n",
    "query_vec = [v + random.uniform(-0.09, 0.09) for v in vectors[\"vec10\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lsh_index.query(query_vec, top_k=5)\n",
    "\n",
    "print(\"Nearest neighbors:\")\n",
    "print(\"-\" * 50)\n",
    "for result in results:\n",
    "    print(f\"{result['id']}: similarity = {result['similarity']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Implementing a Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Vector Database Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VecorDatabase:\n",
    "\n",
    "    DIM = 1536  # Default dimension for OpenAI's text-embedding-3-small\n",
    "\n",
    "    def __init__(self, num_tables=2, num_hashes=3):\n",
    "        self.openai_client = OpenAI(api_key=OPENAI_API_KEY) # OpenAI client for generating embeddings\n",
    "        self.lsh_index = LSH(dim=self.DIM, num_tables=num_tables, num_hashes=num_hashes)\n",
    "        self.data = {}\n",
    "\n",
    "    def _get_embedding(self, text):\n",
    "        return self.openai_client.embeddings.create(model=\"text-embedding-3-small\",input=text).data[0].embedding\n",
    "\n",
    "    def upsert(self, id, text, metadata=dict()):\n",
    "        vector = self._get_embedding(text)\n",
    "        self.data[id] = {\"vector\": vector, \"text\": text, \"metadata\": metadata}\n",
    "        self.lsh_index.insert(id, vector)\n",
    "\n",
    "    def search(self, query_text, top_k=3):   \n",
    "        query_vec = self._get_embedding(query_text)\n",
    "        results = self.lsh_index.query(query_vec, top_k)\n",
    "        enriched = []\n",
    "        for result in results:\n",
    "            id = result[\"id\"]\n",
    "            similarity = round(result[\"similarity\"], 4)\n",
    "            record = self.data.get(id)  # get the record from the data dictionary\n",
    "            if record:\n",
    "                enriched.append({\n",
    "                    \"id\": id,\n",
    "                    \"similarity\": similarity,\n",
    "                    \"text\": record[\"text\"],\n",
    "                    \"metadata\": record[\"metadata\"]\n",
    "                })\n",
    "        return enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = VecorDatabase(num_hashes=2, num_tables=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation for Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    { \"_id\": \"rec1\", \"chunk_text\": \"The Eiffel Tower was completed in 1889 and stands in Paris, France.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec2\", \"chunk_text\": \"Photosynthesis allows plants to convert sunlight into energy.\", \"category\": \"science\" },\n",
    "    { \"_id\": \"rec3\", \"chunk_text\": \"Albert Einstein developed the theory of relativity.\", \"category\": \"science\" },\n",
    "    { \"_id\": \"rec4\", \"chunk_text\": \"The mitochondrion is often called the powerhouse of the cell.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec5\", \"chunk_text\": \"Shakespeare wrote many famous plays, including Hamlet and Macbeth.\", \"category\": \"literature\" },\n",
    "    { \"_id\": \"rec6\", \"chunk_text\": \"Water boils at 100¬∞C under standard atmospheric pressure.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec7\", \"chunk_text\": \"The Great Wall of China was built to protect against invasions.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec8\", \"chunk_text\": \"Honey never spoils due to its low moisture content and acidity.\", \"category\": \"food science\" },\n",
    "    { \"_id\": \"rec9\", \"chunk_text\": \"The speed of light in a vacuum is approximately 299,792 km/s.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec10\", \"chunk_text\": \"Newton's laws describe the motion of objects.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec11\", \"chunk_text\": \"The human brain has approximately 86 billion neurons.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec12\", \"chunk_text\": \"The Amazon Rainforest is one of the most biodiverse places on Earth.\", \"category\": \"geography\" },\n",
    "    { \"_id\": \"rec13\", \"chunk_text\": \"Black holes have gravitational fields so strong that not even light can escape.\", \"category\": \"astronomy\" },\n",
    "    { \"_id\": \"rec14\", \"chunk_text\": \"The periodic table organizes elements based on their atomic number.\", \"category\": \"chemistry\" },\n",
    "    { \"_id\": \"rec15\", \"chunk_text\": \"Leonardo da Vinci painted the Mona Lisa.\", \"category\": \"art\" },\n",
    "    { \"_id\": \"rec16\", \"chunk_text\": \"The internet revolutionized communication and information sharing.\", \"category\": \"technology\" },\n",
    "    { \"_id\": \"rec17\", \"chunk_text\": \"The Pyramids of Giza are among the Seven Wonders of the Ancient World.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec18\", \"chunk_text\": \"Dogs have an incredible sense of smell, much stronger than humans.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec19\", \"chunk_text\": \"The Pacific Ocean is the largest and deepest ocean on Earth.\", \"category\": \"geography\" },\n",
    "    { \"_id\": \"rec20\", \"chunk_text\": \"Chess is a strategic game that originated in India.\", \"category\": \"games\" },\n",
    "    { \"_id\": \"rec21\", \"chunk_text\": \"The Statue of Liberty was a gift from France to the United States.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec22\", \"chunk_text\": \"Coffee contains caffeine, a natural stimulant.\", \"category\": \"food science\" },\n",
    "    { \"_id\": \"rec23\", \"chunk_text\": \"Thomas Edison invented the practical electric light bulb.\", \"category\": \"inventions\" },\n",
    "    { \"_id\": \"rec24\", \"chunk_text\": \"The moon influences ocean tides due to gravitational pull.\", \"category\": \"astronomy\" },\n",
    "    { \"_id\": \"rec25\", \"chunk_text\": \"DNA carries genetic information for all living organisms.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec26\", \"chunk_text\": \"Rome was once the center of a vast empire.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec27\", \"chunk_text\": \"The Wright brothers pioneered human flight in 1903.\", \"category\": \"inventions\" },\n",
    "    { \"_id\": \"rec28\", \"chunk_text\": \"Bananas are a good source of potassium.\", \"category\": \"nutrition\" },\n",
    "    { \"_id\": \"rec29\", \"chunk_text\": \"The stock market fluctuates based on supply and demand.\", \"category\": \"economics\" },\n",
    "    { \"_id\": \"rec30\", \"chunk_text\": \"A compass needle points toward the magnetic north pole.\", \"category\": \"navigation\" },\n",
    "    { \"_id\": \"rec31\", \"chunk_text\": \"The universe is expanding, according to the Big Bang theory.\", \"category\": \"astronomy\" },\n",
    "    { \"_id\": \"rec32\", \"chunk_text\": \"Elephants have excellent memory and strong social bonds.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec33\", \"chunk_text\": \"The violin is a string instrument commonly used in orchestras.\", \"category\": \"music\" },\n",
    "    { \"_id\": \"rec34\", \"chunk_text\": \"The heart pumps blood throughout the human body.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec35\", \"chunk_text\": \"Ice cream melts when exposed to heat.\", \"category\": \"food science\" },\n",
    "    { \"_id\": \"rec36\", \"chunk_text\": \"Solar panels convert sunlight into electricity.\", \"category\": \"technology\" },\n",
    "    { \"_id\": \"rec37\", \"chunk_text\": \"The French Revolution began in 1789.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec38\", \"chunk_text\": \"The Taj Mahal is a mausoleum built by Emperor Shah Jahan.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec39\", \"chunk_text\": \"Rainbows are caused by light refracting through water droplets.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec40\", \"chunk_text\": \"Mount Everest is the tallest mountain in the world.\", \"category\": \"geography\" },\n",
    "    { \"_id\": \"rec41\", \"chunk_text\": \"Octopuses are highly intelligent marine creatures.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec42\", \"chunk_text\": \"The speed of sound is around 343 meters per second in air.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec43\", \"chunk_text\": \"Gravity keeps planets in orbit around the sun.\", \"category\": \"astronomy\" },\n",
    "    { \"_id\": \"rec44\", \"chunk_text\": \"The Mediterranean diet is considered one of the healthiest in the world.\", \"category\": \"nutrition\" },\n",
    "    { \"_id\": \"rec45\", \"chunk_text\": \"A haiku is a traditional Japanese poem with a 5-7-5 syllable structure.\", \"category\": \"literature\" },\n",
    "    { \"_id\": \"rec46\", \"chunk_text\": \"The human body is made up of about 60% water.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec47\", \"chunk_text\": \"The Industrial Revolution transformed manufacturing and transportation.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec48\", \"chunk_text\": \"Vincent van Gogh painted Starry Night.\", \"category\": \"art\" },\n",
    "    { \"_id\": \"rec49\", \"chunk_text\": \"Airplanes fly due to the principles of lift and aerodynamics.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec50\", \"chunk_text\": \"Renewable energy sources include wind, solar, and hydroelectric power.\", \"category\": \"energy\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert facts into the database\n",
    "for record in records:\n",
    "    db.upsert(\n",
    "        id=record[\"_id\"],\n",
    "        text=record[\"chunk_text\"],\n",
    "        metadata={\"category\": record[\"category\"]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, v = list(db.data.items())[1]\n",
    "print(k)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.lsh_index.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_text = \"Famous historical structures and monuments\"\n",
    "results = db.search(search_text, top_k=10)\n",
    "\n",
    "print(\"\\nüîç Search Results:\")\n",
    "for r in results:\n",
    "    print(f\"ID: {r['id']}, Similarity: {r['similarity']}, Text: {r['text']}, Metadata: {r['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Reranking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector search retrieves the top N most similar documents based on embeddings, but those results are not always the most relevant to the specific query. Reranking is a technique that takes the initial list of retrieved results and reorders them based on a deeper, more accurate relevance assessment to the user‚Äôs query.\n",
    "\n",
    "> A reranking model scores and reorders the returned candidates using a deeper semantic comparison, ensuring the final documents are more contextually accurate. This leads to noticeably better answer quality and fewer hallucinations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector DB stores vectors and uses nearest-neighbor search. So when you query it, it says:\n",
    "\n",
    "‚ÄúWhich items are closest in vector space to this query embedding?‚Äù. But embeddings are not perfect:\n",
    "- They capture general meaning\n",
    "- They can get confused by wording\n",
    "- They may return things that are related but not contextually correct\n",
    "\n",
    "**Example**\n",
    "\n",
    "Query: ‚ÄúHow do I change my billing address?‚Äù\n",
    "\n",
    "Top vector results might include:\n",
    "- ‚ÄúBilling API documentation‚Äù (similar word surface form)\n",
    "- ‚ÄúHow to change your email address‚Äù (similar topic)\n",
    "- ‚ÄúInvoice generation‚Äù (same semantic domain)\n",
    "\n",
    "They‚Äôre close, but not all are truly what you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Reranking Does**\n",
    "\n",
    "Reranking takes the top N results (e.g., top 20 or 50) and runs a more expensive but more accurate model (like a cross-encoder) to evaluate:\n",
    "\n",
    "‚ÄúGiven the query and each individual document, how relevant is this really?‚Äù\n",
    "\n",
    "This second (reranking) model compares the pair (query, document) directly. So it‚Äôs more precise, but too slow to run on the whole database.\n",
    "\n",
    "The Two-Stage Workflow\n",
    "- Vector search: fast filter ‚Üí gets 20 candidates\n",
    "- Reranker: slow but precise ‚Üí sorts those 20 accurately\n",
    "\n",
    "This gives you speed and quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Introduction to Pinecone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector databases are specialized systems designed to store, index, and search high-dimensional vectors (embeddings) efficiently. As AI applications such as semantic search, recommendation systems, and Retrieval-Augmented Generation (RAG) have grown in adoption, a rich ecosystem of vector databases has emerged‚Äîeach offering different trade-offs in performance, scalability, deployment model, and developer experience.\n",
    "\n",
    "Popular solutions in this space include open-source options like **Pinecone**, **Qdrant**, **Weaviate**, **Milvus**, and **Chroma**, as well as fully managed cloud services. While these databases share the same core goal‚Äîfast and accurate vector similarity search‚Äîthey differ in how much infrastructure management, scaling, and operational complexity they place on the user.\n",
    "\n",
    "**Pinecone** stands out as a fully managed, cloud-native vector database designed specifically for production AI workloads. It abstracts away infrastructure concerns such as indexing strategies, sharding, and scaling, allowing developers to focus on building AI features rather than operating databases. Pinecone offers low-latency similarity search, rich metadata filtering, and tight integration with modern embedding and LLM workflows, making it a strong choice for real-world semantic search and RAG systems.\n",
    "\n",
    "In this section, we will focus on Pinecone‚Äôs core concepts, architecture, and usage patterns, while keeping in mind how it fits into the broader vector database landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Feature**                       | **Pinecone**                     | **Qdrant**           | **Weaviate**             | **Milvus**          | **Chroma**          |\n",
    "| --------------------------------- | -------------------------------- | -------------------- | ------------------------ | ------------------- | ------------------- |\n",
    "| **Type**                          | Managed Cloud                    | Open Source / Cloud  | Open Source / Cloud      | Open Source         | Local / Lightweight |\n",
    "| **Ease of Setup**                 | ‚≠ê‚≠ê‚≠ê‚≠ê (very easy)                 | ‚≠ê‚≠ê                   | ‚≠ê‚≠ê                       | ‚≠ê                   | ‚≠ê‚≠ê‚≠ê‚≠ê                |\n",
    "| **Performance**                   | ‚≠ê‚≠ê‚≠ê‚≠ê                             | ‚≠ê‚≠ê‚≠ê‚≠ê                 | ‚≠ê‚≠ê‚≠ê                      | ‚≠ê‚≠ê‚≠ê‚≠ê                | ‚≠ê‚≠ê                  |\n",
    "| **Scalability**                   | ‚≠ê‚≠ê‚≠ê‚≠ê                             | ‚≠ê‚≠ê‚≠ê                  | ‚≠ê‚≠ê‚≠ê‚≠ê                     | ‚≠ê‚≠ê‚≠ê‚≠ê                | ‚≠ê                   |\n",
    "| **Metadata Filtering**            | ‚úÖ                                | ‚úÖ                    | ‚úÖ                        | ‚úÖ                   | ‚úÖ                   |\n",
    "| **Hybrid Search (Text + Vector)** | ‚ùå                                | ‚úÖ                    | ‚úÖ                        | ‚úÖ                   | ‚ùå                   |\n",
    "| **Best For**                      | Production apps, managed scaling | Open-source projects | Semantic + hybrid search | Research / Big data | Rapid prototyping   |\n",
    "| **License / Pricing**             | Commercial (free tier)           | Apache 2.0           | BSD                      | Apache 2.0          | MIT                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 7.1. Pinecone Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pinecone is a managed vector database designed to store, index, and query high-dimensional vectors at scale. It handles the heavy lifting of infrastructure, scalability, and performance, so you can focus on building AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each project contains a single `database`\n",
    "- A database can include multiple `indexes`\n",
    "\n",
    "> Typically, you create one index per use case. For Example: different indexes for different embedding models or retrieval strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within an index, records are partitioned into `namespaces`, and all upserts, queries, and other data operations always target one namespace. This has two main benefits:\n",
    "  - Multitenancy: When you need to isolate data between customers, you can use one namespace per customer and target each customer‚Äôs writes and queries to their dedicated namespace. See Implement multitenancy for end-to-end guidance.\n",
    "  - Faster queries: When you divide records into namespaces in a logical way, you speed up queries by ensuring only relevant records are scanned. The same applies to fetching records, listing record IDs, and other data operations.\n",
    "\n",
    "Namespaces are created automatically during upsert. If a namespace doesn‚Äôt exist, it is created implicitly. \n",
    "\n",
    "> There is no mechanism offered by Pinecone to search (or do any operation) across multiple namespaces\n",
    "\n",
    "[check](https://www.youtube.com/shorts/gp5bFF4QNCQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense vs Sparse Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pinecone index can store dense vectors, sparse vectors, or both, enabling semantic, keyword, or hybrid search within the same index.\n",
    "\n",
    "**Dense Embeddings ‚Äî Use Cases**\n",
    "Dense vectors capture semantic meaning and are typically produced by embedding models like OpenAI, BGE, or CLIP.\n",
    "- Semantic search ‚Äì find documents or passages based on meaning, not exact keywords\n",
    "- Similarity search ‚Äì find similar texts, images, or products\n",
    "- Recommendation systems ‚Äì recommend content or items based on user behavior or preferences\n",
    "- RAG (Retrieval-Augmented Generation) ‚Äì retrieve relevant context to enrich LLM responses\n",
    "- Multimodal search ‚Äì search images using text or other images (e.g., OpenCLIP)\n",
    "üëâ Best when understanding meaning and context matters more than exact wording.\n",
    "\n",
    "**Sparse Embeddings ‚Äî Use Cases**\n",
    "Sparse vectors represent data where most values are zero and usually correspond to keyword-based or token-based representations (e.g., BM25, TF-IDF).\n",
    "- Keyword search ‚Äì exact or near-exact term matching\n",
    "- Filtering by rare or specific terms ‚Äì names, IDs, error codes, part numbers\n",
    "- Search in technical or legal documents where exact wording matters\n",
    "- Explainable search results ‚Äì easier to understand why a result matched\n",
    "üëâ Best when precision and exact matches are critical.\n",
    "\n",
    "**Hybrid Search ‚Äî Use Cases**\n",
    "Hybrid search combines dense semantic understanding with sparse keyword precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pinecone Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record stored in Pinecone consists of three main components:\n",
    "- ID ‚Äì a unique identifier for the vector\n",
    "- Vector ‚Äì the numerical embedding representing meaning\n",
    "- Metadata ‚Äì additional structured information used for filtering and context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 7.2. Initializing Pinecone Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pinecone is a fully managed, cloud-based vector database. It‚Äôs easy to set up, requires no server management, and integrates seamlessly with OpenAI embeddings and LangChain ‚Äî making it perfect for a hands-on learning experience.\n",
    "\n",
    "> Setup the Database and get your API Key from [here](https://www.pinecone.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Creating an Index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pinecone, there are two types of indexes for storing vector data: `Dense` indexes store dense vectors for `semantic` search, and `sparse` indexes store sparse vectors for `lexical/keyword` search. For our example, we will create a `dense` index \n",
    "\n",
    "To convert the input data into embeddings (i.e. vector format), Pinecone offers integrated embedding models (e.g. `llama-text-embed-v2`) that can be used directly out of the box. Alternatively, an external embedding model can be used (ex. Open AI's `text-embedding-3-small`) and store the vector data directly to Pinecone. For this example, we will use the integrated model `llama-text-embed-v2`. This means, when we upsert and search our data, Pinecone will autimatically generate the vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Manage Embedding Externally | Manage embedding within DB |\n",
    "|----------------------------|----------------------------|\n",
    "| You generate embeddings in your application code using an external model or service, explicitly define vector dimensions and metrics, and manually upsert vectors into the index. This gives you full control over model choice, versioning, preprocessing, and embedding lifecycle, but requires extra infrastructure and code to manage embedding generation and updates. | The database automatically generates embeddings using a built-in model based on a mapped text field. You don‚Äôt manage vector dimensions or embedding generation directly, which simplifies the pipeline and reduces boilerplate, but ties you to the database-supported models and limits control over embedding customization and versioning. |\n",
    "| ![Picture 1](imgs/pc_index_external_model.png) | ![Picture 2](imgs/pc_index_managed_model.png) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dense index with integrated embedding\n",
    "index_name = \"my-index\"\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index_for_model(\n",
    "        name=index_name,\n",
    "        # specify where the index will be hosted (cloud provider and region)\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "        \n",
    "        # specify the embedding model to use\n",
    "        embed={\n",
    "            \"model\":\"llama-text-embed-v2\",\n",
    "            # specify the field to embed\n",
    "            \"field_map\":{\"text\": \"chunk_text\"}\n",
    "        }, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Data Preparation & Insertion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a sample dataset of factual statements from different domains like history, physics, technology, and music. Model the data as as records with an ID, text, and category.\n",
    "\n",
    "> As you will notice below, each record have a unique `_id` field, and the field `chunk_text`; `chunk_text` was defined on index creation as the field to be used for embedding (i.e. the field that will be transfored into a vector representation). All additional fields are stored as record metadata. We can filter by metadata when searching or deleting records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    { \"_id\": \"rec1\", \"chunk_text\": \"The Eiffel Tower was completed in 1889 and stands in Paris, France.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec2\", \"chunk_text\": \"Photosynthesis allows plants to convert sunlight into energy.\", \"category\": \"science\" },\n",
    "    { \"_id\": \"rec3\", \"chunk_text\": \"Albert Einstein developed the theory of relativity.\", \"category\": \"science\" },\n",
    "    { \"_id\": \"rec4\", \"chunk_text\": \"The mitochondrion is often called the powerhouse of the cell.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec5\", \"chunk_text\": \"Shakespeare wrote many famous plays, including Hamlet and Macbeth.\", \"category\": \"literature\" },\n",
    "    { \"_id\": \"rec6\", \"chunk_text\": \"Water boils at 100¬∞C under standard atmospheric pressure.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec7\", \"chunk_text\": \"The Great Wall of China was built to protect against invasions.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec8\", \"chunk_text\": \"Honey never spoils due to its low moisture content and acidity.\", \"category\": \"food science\" },\n",
    "    { \"_id\": \"rec9\", \"chunk_text\": \"The speed of light in a vacuum is approximately 299,792 km/s.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec10\", \"chunk_text\": \"Newton's laws describe the motion of objects.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec11\", \"chunk_text\": \"The human brain has approximately 86 billion neurons.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec12\", \"chunk_text\": \"The Amazon Rainforest is one of the most biodiverse places on Earth.\", \"category\": \"geography\" },\n",
    "    { \"_id\": \"rec13\", \"chunk_text\": \"Black holes have gravitational fields so strong that not even light can escape.\", \"category\": \"astronomy\" },\n",
    "    { \"_id\": \"rec14\", \"chunk_text\": \"The periodic table organizes elements based on their atomic number.\", \"category\": \"chemistry\" },\n",
    "    { \"_id\": \"rec15\", \"chunk_text\": \"Leonardo da Vinci painted the Mona Lisa.\", \"category\": \"art\" },\n",
    "    { \"_id\": \"rec16\", \"chunk_text\": \"The internet revolutionized communication and information sharing.\", \"category\": \"technology\" },\n",
    "    { \"_id\": \"rec17\", \"chunk_text\": \"The Pyramids of Giza are among the Seven Wonders of the Ancient World.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec18\", \"chunk_text\": \"Dogs have an incredible sense of smell, much stronger than humans.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec19\", \"chunk_text\": \"The Pacific Ocean is the largest and deepest ocean on Earth.\", \"category\": \"geography\" },\n",
    "    { \"_id\": \"rec20\", \"chunk_text\": \"Chess is a strategic game that originated in India.\", \"category\": \"games\" },\n",
    "    { \"_id\": \"rec21\", \"chunk_text\": \"The Statue of Liberty was a gift from France to the United States.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec22\", \"chunk_text\": \"Coffee contains caffeine, a natural stimulant.\", \"category\": \"food science\" },\n",
    "    { \"_id\": \"rec23\", \"chunk_text\": \"Thomas Edison invented the practical electric light bulb.\", \"category\": \"inventions\" },\n",
    "    { \"_id\": \"rec24\", \"chunk_text\": \"The moon influences ocean tides due to gravitational pull.\", \"category\": \"astronomy\" },\n",
    "    { \"_id\": \"rec25\", \"chunk_text\": \"DNA carries genetic information for all living organisms.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec26\", \"chunk_text\": \"Rome was once the center of a vast empire.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec27\", \"chunk_text\": \"The Wright brothers pioneered human flight in 1903.\", \"category\": \"inventions\" },\n",
    "    { \"_id\": \"rec28\", \"chunk_text\": \"Bananas are a good source of potassium.\", \"category\": \"nutrition\" },\n",
    "    { \"_id\": \"rec29\", \"chunk_text\": \"The stock market fluctuates based on supply and demand.\", \"category\": \"economics\" },\n",
    "    { \"_id\": \"rec30\", \"chunk_text\": \"A compass needle points toward the magnetic north pole.\", \"category\": \"navigation\" },\n",
    "    { \"_id\": \"rec31\", \"chunk_text\": \"The universe is expanding, according to the Big Bang theory.\", \"category\": \"astronomy\" },\n",
    "    { \"_id\": \"rec32\", \"chunk_text\": \"Elephants have excellent memory and strong social bonds.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec33\", \"chunk_text\": \"The violin is a string instrument commonly used in orchestras.\", \"category\": \"music\" },\n",
    "    { \"_id\": \"rec34\", \"chunk_text\": \"The heart pumps blood throughout the human body.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec35\", \"chunk_text\": \"Ice cream melts when exposed to heat.\", \"category\": \"food science\" },\n",
    "    { \"_id\": \"rec36\", \"chunk_text\": \"Solar panels convert sunlight into electricity.\", \"category\": \"technology\" },\n",
    "    { \"_id\": \"rec37\", \"chunk_text\": \"The French Revolution began in 1789.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec38\", \"chunk_text\": \"The Taj Mahal is a mausoleum built by Emperor Shah Jahan.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec39\", \"chunk_text\": \"Rainbows are caused by light refracting through water droplets.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec40\", \"chunk_text\": \"Mount Everest is the tallest mountain in the world.\", \"category\": \"geography\" },\n",
    "    { \"_id\": \"rec41\", \"chunk_text\": \"Octopuses are highly intelligent marine creatures.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec42\", \"chunk_text\": \"The speed of sound is around 343 meters per second in air.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec43\", \"chunk_text\": \"Gravity keeps planets in orbit around the sun.\", \"category\": \"astronomy\" },\n",
    "    { \"_id\": \"rec44\", \"chunk_text\": \"The Mediterranean diet is considered one of the healthiest in the world.\", \"category\": \"nutrition\" },\n",
    "    { \"_id\": \"rec45\", \"chunk_text\": \"A haiku is a traditional Japanese poem with a 5-7-5 syllable structure.\", \"category\": \"literature\" },\n",
    "    { \"_id\": \"rec46\", \"chunk_text\": \"The human body is made up of about 60% water.\", \"category\": \"biology\" },\n",
    "    { \"_id\": \"rec47\", \"chunk_text\": \"The Industrial Revolution transformed manufacturing and transportation.\", \"category\": \"history\" },\n",
    "    { \"_id\": \"rec48\", \"chunk_text\": \"Vincent van Gogh painted Starry Night.\", \"category\": \"art\" },\n",
    "    { \"_id\": \"rec49\", \"chunk_text\": \"Airplanes fly due to the principles of lift and aerodynamics.\", \"category\": \"physics\" },\n",
    "    { \"_id\": \"rec50\", \"chunk_text\": \"Renewable energy sources include wind, solar, and hydroelectric power.\", \"category\": \"energy\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert/Upsert "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will insert/upsert the data into our Index. Because we setup the index with an integrated embedding model, we only provide the textual statements (via the field `chunk_text`) and Pinecone converts them to dense vectors automatically.\n",
    "\n",
    "> Notice that Pinecone is eventually consistent, so there can be a slight delay before new or changed records are visible to queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target the index\n",
    "dense_index = pc.Index(index_name)\n",
    "\n",
    "# Upsert the records into a namespace\n",
    "namespace_name = \"my-namespace\"\n",
    "dense_index.upsert_records(namespace_name, records)\n",
    "\n",
    "# Wait for the upserted vectors to be indexed\n",
    "import time\n",
    "time.sleep(10)\n",
    "\n",
    "# View stats for the index\n",
    "stats = dense_index.describe_index_stats()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Index Query & Reranking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index Query: Searching Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we search the dense index for records that are most semantically similar to the query, ‚ÄúFamous historical structures and monuments‚Äù. Again, because the index is setup with an integrated embedding model, we provide the query as text and Pinecone converts the text to a dense vector automatically.\n",
    "\n",
    "> Notice that we specify the `Namespace` we would like to perform the search operation on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "query = \"Famous historical structures and monuments\"\n",
    "\n",
    "# Search the dense index\n",
    "results = dense_index.search(\n",
    "    namespace=namespace_name,\n",
    "    query={\n",
    "        \"top_k\": 10,\n",
    "        \"inputs\": {\n",
    "            'text': query\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for hit in results['result']['hits']:\n",
    "        print(f\"id: {hit['_id']:<5} | score: {round(hit['_score'], 2):<5} | category: {hit['fields']['category']:<10} | text: {hit['fields']['chunk_text']:<50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector databases retrieve results based on embedding similarity (usually cosine or dot-product distance). While this works well for semantic closeness, the top-ranked vectors aren‚Äôt always the most relevant in context ‚Äî especially when:\n",
    "  - The query is ambiguous (e.g., ‚ÄúApple‚Äù could mean the fruit or the company).\n",
    "  - The embeddings miss fine-grained meaning (subtle differences between retrieved results).\n",
    "  - The user‚Äôs intent isn‚Äôt purely semantic (e.g., preferring recent, local, or authoritative content).\n",
    "\n",
    "As a result, raw vector search may return items that are similar in meaning but not optimal in relevance.\n",
    "\n",
    "> Re-ranking improves search quality by taking the initial list of retrieved documents (from the vector DB) and ordering them more intelligently. It typically uses a secondary model (like a cross-encoder or LLM) that compares the query with each result directly to predict a more accurate relevance score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a more accurate ranking, we can rerank the initial results based on their relevance to the query. Reranking is used as part of a two-stage vector retrieval process to improve the quality of results.\n",
    "  - First we query an index for a given number of relevant results\n",
    "  - Second we send the query and results to a reranking model.\n",
    "\n",
    "The reranking model scores the results based on their semantic relevance to the query and returns a new, more accurate ranking. This approach is one of the simplest methods for improving quality in retrieval augmented generation (RAG) pipelines.\n",
    "\n",
    "Pinecone provides hosted reranking models so it‚Äôs easy to manage two-stage vector retrieval via the same platform. It is possible to use a hosted model to rerank results as an integrated part of a query, or we can use a hosted model or external model to rerank results as a standalone operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the dense index and rerank results\n",
    "reranked_results = dense_index.search(\n",
    "    namespace=namespace_name,\n",
    "    query={\n",
    "        \"top_k\": 10,\n",
    "        \"inputs\": {\n",
    "            'text': query\n",
    "        }\n",
    "    },\n",
    "    rerank={\n",
    "        \"model\": \"bge-reranker-v2-m3\",\n",
    "        \"top_n\": 10,\n",
    "        \"rank_fields\": [\"chunk_text\"]\n",
    "    }   \n",
    ")\n",
    "\n",
    "# Print the reranked results\n",
    "for hit in reranked_results['result']['hits']:\n",
    "    print(f\"id: {hit['_id']}, score: {round(hit['_score'], 2)}, text: {hit['fields']['chunk_text']}, category: {hit['fields']['category']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reranking results is one of the most effective ways to improve search accuracy and relevance, but there are many other techniques to consider. For example:\n",
    "  - Filtering by metadata: When records contain additional metadata, you can limit the search to records matching a filter expression.\n",
    "  - Hybrid search: You can add lexical search to capture precise keyword matches (e.g., product SKUs, email addresses, domain-specific terms) in addition to semantic matches.\n",
    "  - Chunking strategies: You can chunk your content in different ways to get better results. Consider factors like the length of the content, the complexity of queries, and how results will be used in your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6. Clean Up !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up our resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Practical Usecases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Usecase: Image Search and Similar Images Recommendation Using Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we‚Äôve mainly worked with text ‚Äî turning words into vectors and searching for meaning. In this section, we take a big step forward and move into something even more powerful: multimodal search.\n",
    "\n",
    "We‚Äôll use `OpenCLIP` - an embedding model based on Open AIs [CLIP](https://openai.com/index/clip/) to implement two very interesting use-cases\n",
    "\n",
    "- Search Images by providing an input `text` query\n",
    "- Search Visiually Similar Images to a given input `Image` as query\n",
    "\n",
    "OpenCLIP uses the same embedding model to convert both images and text into vectors in a shared space. This allows us to do something that feels almost magical: search for images using text, or find similar images using another image ‚Äî all with the same vector database.\n",
    "\n",
    "\n",
    "You‚Äôll see how an image or a short phrase can be embedded into the same vector space, indexed once, and queried in multiple ways. This is a perfect example of how vector databases enable flexible, real-world AI systems ‚Äî not just single-modality search, but cross-modal understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch open_clip_torch langchain-experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxilary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        return image_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "def image_to_base64(img):\n",
    "    encoded = base64.b64encode(img)\n",
    "    return encoded.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires that open_clip_torch is installed\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below initializes an OpenCLIP embedding model that we will use to convert images and text into vector representations.\n",
    "\n",
    "- model_name = \"ViT-g-14\" specifies the architecture of the image encoder.`ViT-g-14` is a large Vision Transformer model that produces high-quality visual embeddings, making it well-suited for similarity search and retrieval tasks.\n",
    "\n",
    "- checkpoint = \"laion2b_s34b_b88k\" tells OpenCLIP which pretrained weights to load. This checkpoint was trained on a massive dataset of image‚Äìtext pairs `(LAION-2B)`, allowing the model to understand rich visual and semantic relationships.\n",
    "\n",
    "- OpenCLIPEmbeddings(...) creates an embedding object that can embed both images and text into the same vector space. This is what enables multimodal search, such as finding images using text or finding similar images using another image.\n",
    "\n",
    "After this step, clip_embd can be used to generate embeddings that we store in a vector database and query for similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ViT-g-14\"\n",
    "checkpoint = \"laion2b_s34b_b88k\"\n",
    "clip_embd = OpenCLIPEmbeddings(model_name=model_name, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"resources/fashion\"\n",
    "categories = [\"jackets\", \"shirts\", \"shoes\"]\n",
    "all_images = dict()\n",
    "idx = 0\n",
    "\n",
    "for category in categories:\n",
    "    dir_path = f\"{root_path}/{category}\"\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        idx += 1\n",
    "        img_id = f\"img{idx}\"\n",
    "        file_path = f\"{dir_path}/{file_name}\"\n",
    "        img = read_img(file_path)\n",
    "        #\n",
    "        all_images[img_id] = {\n",
    "            \"file_name\": file_name,\n",
    "            \"file_path\": file_path,\n",
    "            \"img\": img,\n",
    "            \"img_b64\": image_to_base64(img),\n",
    "            \"category\": category\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = list(all_images.values())[0]\n",
    "img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = list()\n",
    "for img_id, img_data in all_images.items():\n",
    "    img_embedding = clip_embd.embed_image([img_data['file_path']])[0]\n",
    "    records.append({\n",
    "        \"id\": img_id,\n",
    "        \"values\": img_embedding, \n",
    "        \"metadata\": {\"category\": img_data[\"category\"]}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(records[0]['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm(records[0]['values']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating our Index & Data Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"my-index\"\n",
    "# Create a dense index\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ),\n",
    "        vector_type=\"dense\",\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\"       \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target the index\n",
    "dense_index = pc.Index(index_name)\n",
    "namespace_name = \"my-namespace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_index.upsert(vectors=records, namespace=namespace_name)\n",
    "\n",
    "# Wait for the upserted vectors to be indexed\n",
    "import time\n",
    "time.sleep(10)\n",
    "\n",
    "# View stats for the index\n",
    "stats = dense_index.describe_index_stats()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Querying our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_html_by_vector(query_vector, top_k):\n",
    "\n",
    "    # get images\n",
    "    query_response = dense_index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=top_k,\n",
    "        namespace=namespace_name,\n",
    "        include_metadata=False\n",
    "    )\n",
    "\n",
    "    # generat html for display\n",
    "    html_str = \"\"\n",
    "    for match in query_response['matches']:\n",
    "        iid = match['id']\n",
    "        ib64 = all_images[iid][\"img_b64\"]\n",
    "        html_str += f'<img src=\"data:image/png;base64,{ib64}\" width=\"100\" height=\"100\" style=\"margin-right:10px\"/>'\n",
    "\n",
    "    return html_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs begin with a text-based search, where we describe what we‚Äôre looking for in words and let the model retrieve the most similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"checked shirt\"\n",
    "query_vector = clip_embd.embed_documents([query_text])[0]\n",
    "display_html = get_images_html_by_vector(query_vector, top_k=5)\n",
    "display(HTML(display_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we‚Äôll switch to image-based search, where the query itself is an image and we retrieve visually similar images from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img_path = \"resources/fashion/jackets/IBBW23-5553_999_1_9c6e4cd2-8819-41ee-b6d5-aaf7adff3c4a.webp\"\n",
    "query_img = read_img(query_img_path)\n",
    "Image(data=query_img, width=\"100\", height=\"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = clip_embd.embed_image([query_img_path])[0]\n",
    "display_html = get_images_html_by_vector(query_vector, top_k=5)\n",
    "display(HTML(display_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean Up !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Usecase: Retrieval-Augmented Generation for AI Customer Support Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we show how vector databases can be used to build a RAG pipeline. We demonstrate this by illustrating how a customer support agent (LLM) can improve its responses by connecting to an external knowledge base (a vector database). This example assumes prior knowledge of how LLMs work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Customer Support Agent operates within a sneaker store, assisting customers with a wide range of inquiries. It can answer questions about order returns, shipping status, account management, product availability, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval-augmented generation (RAG)** is a technique used to enhance the accuracy and reliability of generative AI models by incorporating information retrieved from relevant external data sources.\n",
    "\n",
    "Without RAG, an LLM generates responses based solely on the knowledge it acquired during training. With RAG, an information retrieval component is added: the user‚Äôs query is first used to retrieve relevant information from an external data source. Both the user query and the retrieved information are then provided to the LLM, allowing it to combine its training knowledge with up-to-date or domain-specific data to produce more accurate and reliable responses.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/rag.jpg\" alt=\"Alt text\" width=\"600\" height=\"350\" center>\n",
    "</div>\n",
    "\n",
    "Image credit is from [this](https://aws.amazon.com/what-is/retrieval-augmented-generation/) amazing article from AWS. Feel free to read it for a nice introduction to RAGs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets Initialize Our Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Customer Support Agent leverages previous human-led support conversations, handled by business-aware employees, to gain contextual understanding of the business and provide more informed answers. To enable this, all past conversations are ingested into a vector database, allowing for efficient retrieval and contextual grounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets load the input customer support conversations from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"resources/customer_support_conversations.jsonl\"\n",
    "\n",
    "def load_conversations(file_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Loads the conversations from the jsonl file\n",
    "    Args:\n",
    "        file_path: The path to the jsonl file\n",
    "    Returns:\n",
    "        conversations: A list of conversations\n",
    "    \"\"\"\n",
    "\n",
    "    conversations = list()\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        buffer = \"\"\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # skip empty lines\n",
    "            buffer += line\n",
    "            # Check if buffer is a complete JSON object\n",
    "            try:\n",
    "                convo = json.loads(buffer)\n",
    "                conversations.append(convo)\n",
    "                buffer = \"\"  # reset buffer for next object\n",
    "            except json.JSONDecodeError:\n",
    "                # not complete yet, keep reading lines\n",
    "                buffer += \"\\n\"\n",
    "    return conversations\n",
    "\n",
    "\n",
    "conversations = load_conversations(file_path)\n",
    "print(f\"Total conversations loaded: {len(conversations)}\")\n",
    "\n",
    "print(\"Example conversations:\")\n",
    "for conversation in conversations[0:3]:\n",
    "    for message in conversation[\"messages\"]:\n",
    "        print(message)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make each conversation more suitable to insert in the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a single TEXT field that will be given to the Vector database\n",
    "# This will be used for matching the user queries\n",
    "for idx, conversation in enumerate(conversations):\n",
    "    conversation[\"_id\"] = f\"conversation_{idx+1}\"\n",
    "    conversation[\"conversation\"] = \"\\n\".join(conversation[\"messages\"])\n",
    "    conversation.pop(\"messages\")\n",
    "\n",
    "for conversation in conversations[0:3]:\n",
    "    print(f\"Conversation Id: {conversation['_id']}\")\n",
    "    print(conversation[\"conversation\"])\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Lets Create our Index and Insert all our Customer Support Conversations In it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dense index with integrated embedding\n",
    "index_name = \"customer-support-conversations\"\n",
    "namespace_name = \"customer-support-conversations\"\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index_for_model(\n",
    "        name=index_name,\n",
    "        # specify where the index will be hosted (cloud provider and region)\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "        # specify the embedding model to use\n",
    "        embed={\n",
    "            \"model\":\"llama-text-embed-v2\",\n",
    "            # specify the field to embed\n",
    "            \"field_map\":{\"text\": \"conversation\"}\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_into_sublists(lst, n):\n",
    "    \"\"\"\n",
    "    Splits a list `lst` into `n` sublists of roughly equal size.\n",
    "    \"\"\"\n",
    "    k, m = divmod(len(lst), n)\n",
    "    return [lst[i*k + min(i, m):(i+1)*k + min(i+1, m)] for i in range(n)]\n",
    "\n",
    "conversations = split_list_into_sublists (conversations, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target the index\n",
    "dense_index = pc.Index(index_name)\n",
    "\n",
    "# Upsert the records\n",
    "for idx, conversations_subset in enumerate(conversations):\n",
    "    print(f\"Inserting Sublist: {idx+1}\")\n",
    "    dense_index.upsert_records(namespace_name, conversations_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View stats for the index\n",
    "stats = dense_index.describe_index_stats()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Give a Try for the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_context(query):\n",
    "    # Search the dense index\n",
    "    results = dense_index.search(\n",
    "        namespace=namespace_name,\n",
    "        query={\n",
    "            \"top_k\": 3,\n",
    "            \"inputs\": {\n",
    "                'text': query\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for idx, hit in enumerate(results['result']['hits']):\n",
    "        context += f\"Conversation: {idx+1} \\n\"\n",
    "        context += hit['fields']['conversation']\n",
    "        context += \"\\n\\n\"\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "query = \"How long does a shipment take outside of EU\"\n",
    "print(get_relevant_context(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Based Customer Support Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Main interaction\n",
    "query = \"Hello, Am I able to return my Sneakers, I have purchased them 10 days ago\"\n",
    "response = openai.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    input=query, \n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with a RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining agent instructions \n",
    "instructions = \"\"\"\n",
    "You are a customer support agent working for an online sneaker store. Your role is to help customers by \\\n",
    "answering their questions accurately, clearly, and politely.\n",
    "\n",
    "You will be given:\n",
    " - A user query\n",
    " - A context containing relevant information retrieved from past customer support conversations and \\\n",
    "internal knowledge\n",
    "\n",
    "Use only the provided context to answer the user‚Äôs question. If the context does not contain \\\n",
    "enough information to answer confidently, clearly state that you do not have sufficient \\\n",
    "information and suggest an appropriate next step.\n",
    "\n",
    "Your responses should be helpful, concise, and aligned with the store‚Äôs policies and tone of voice.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hello, Am I able to return my Sneakers, I have purchased them 10 days ago\"\n",
    "context = get_relevant_context(query)\n",
    "\n",
    "query_with_context = f\"\"\"\n",
    "Answer the following using the provided context:\n",
    "\n",
    "### Question: \n",
    "{query}\n",
    "\n",
    "### Context: \n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "print(query_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.responses.create(\n",
    "    instructions=instructions, \n",
    "    model=\"gpt-4.1-nano\",\n",
    "    input=query_with_context, \n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\n",
    "# Congratulations !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing the course! You should be really proud of the effort and curiosity you showed. Keep learning, exploring, and believing in yourself‚Äîthis is just the beginning. Wishing you continued success on your learning journey.\n",
    "\n",
    "M. ElSioufy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector-databases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
